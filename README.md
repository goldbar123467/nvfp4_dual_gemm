<div align="center">

```
 â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
 â•‘                                                                              â•‘
 â•‘    â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•—                               â•‘
 â•‘    â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘                               â•‘
 â•‘    â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘                               â•‘
 â•‘    â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â•â• â•šâ•â•â•â•â–ˆâ–ˆâ•‘                               â•‘
 â•‘    â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘          â–ˆâ–ˆâ•‘                               â•‘
 â•‘    â•šâ•â•  â•šâ•â•â•â•  â•šâ•â•â•â•  â•šâ•â•     â•šâ•â•          â•šâ•â•                               â•‘
 â•‘                                                                              â•‘
 â•‘    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—
 â•‘    â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘         â–ˆâ–ˆâ•”â•â•â•â•â• â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘
 â•‘    â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘         â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘
 â•‘    â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘         â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘
 â•‘    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘
 â•‘    â•šâ•â•â•â•â•â•  â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•â•šâ•â•â•â•â•â•â•     â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•â•šâ•â•     â•šâ•â•â•šâ•â•     â•šâ•â•
 â•‘                                                                              â•‘
 â•‘             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â•‘
 â•‘             â”‚                                                 â”‚              â•‘
 â•‘             â”‚      [A] â”€â”€â”€â”€â”¬â”€â”€â”€â”€â–º [B1] â”€â”€â”€â–º SiLU â”€â”€â”          â”‚              â•‘
 â•‘             â”‚              â”‚                       â”œâ”€â”€â–º [C]   â”‚              â•‘
 â•‘             â”‚              â””â”€â”€â”€â”€â–º [B2] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚              â•‘
 â•‘             â”‚                                                 â”‚              â•‘
 â•‘             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â•‘
 â•‘                                                                              â•‘
 â•‘               FP4 Tensor Core Acceleration for Blackwell B200                â•‘
 â•‘                                                                              â•‘
 â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

</div>

<p align="center">
  <a href="#"><img src="https://img.shields.io/badge/LICENSE-MIT-green?style=for-the-badge" alt="License MIT"></a>
  <a href="#"><img src="https://img.shields.io/badge/PYTHON-3.10+-blue?style=for-the-badge&logo=python&logoColor=white" alt="Python 3.10+"></a>
  <a href="#"><img src="https://img.shields.io/badge/CUDA-12+-76B900?style=for-the-badge&logo=nvidia&logoColor=white" alt="CUDA 12+"></a>
  <a href="#"><img src="https://img.shields.io/badge/GPU-B200_BLACKWELL-ff6600?style=for-the-badge&logo=nvidia&logoColor=white" alt="Target GPU B200"></a>
</p>

<p align="center">
  <strong>High-performance fused Dual-GEMM kernel exploiting NVIDIA Blackwell's FP4 Tensor Cores</strong>
</p>

---

## What is this?

This project implements an optimized CUDA kernel for computing the **fused Dual-GEMM operation with SiLU activation**:

```
C = SiLU(A @ B1) * (A @ B2)
```

Targeting **NVIDIA B200 (Blackwell)** GPUs, we leverage the new **NVFP4 (4-bit floating point) Tensor Cores** to achieve maximum throughput for this common pattern found in modern transformer architectures (GLU variants, gated MLPs, etc.).

---

<table align="center">
<tr>
<td align="center">

### ğŸ“Š Performance Summary

| Metric | Value |
|:-------|------:|
| **Target Latency** | 13 Î¼s |
| **Achieved Latency** | ~30 Î¼s |
| **Speedup vs Baseline** | **3.8Ã—** |
| **Precision** | NVFP4 (E2M1) |
| **Architecture** | Blackwell (SM100) |

</td>
</tr>
</table>

---

## Submission Strategies

We explored several optimization approaches to minimize latency. Below is a summary of each strategy, their performance characteristics, and implementation status.

### Performance Comparison

| Strategy | File | Latency | Speedup | Status |
|:---------|:-----|--------:|--------:|:------:|
| Baseline | `submission.py` | ~38Î¼s | 1.0x | âœ… Implemented |
| Cached Scale Factors | `submission_v2.py` | ~70Î¼s | 0.5x | âœ… Implemented |
| CUDA Graphs | `submission_best.py` | ~30Î¼s | **1.3x** | âœ… Implemented |
| Parallel Streams | `submission_streams.py` | ~39Î¼s | ~1.0x | ğŸ”„ Testing |
| Fused Dual-GEMM | â€” | ~13Î¼s | 2.9x | ğŸ“‹ Planned |

---

### Strategy Details

#### 1. âœ… Baseline (`submission.py`) â€” ~38Î¼s

Simple PyTorch implementation leveraging `torch._scaled_mm` for FP4 matrix multiplication.

```python
# Core computation pattern
r1 = torch._scaled_mm(a, b1.T, scale_a, scale_b1, out_dtype=torch.float32)
r2 = torch._scaled_mm(a, b2.T, scale_a, scale_b2, out_dtype=torch.float32)
result = (F.silu(r1) * r2).half()
```

**Characteristics:**
- Two sequential GEMMs followed by SiLU fusion
- No caching â€” recomputes scale factors on every call
- Straightforward and easy to debug

---

#### 2. âœ… Cached Scale Factors (`submission_v2.py`) â€” ~70Î¼s

Optimizes repeated scale factor transformations by caching based on tensor memory addresses.

```python
# Cache lookup by data pointer
key = (sfa_perm.data_ptr(), sfb1_perm.data_ptr(), sfb2_perm.data_ptr(), l_idx)
if key not in self.cache:
    scale_a = sfa_perm[...].permute(2, 4, 0, 1, 3).reshape(-1).clone()
    self.cache[key] = (scale_a, scale_b1, scale_b2)
return self.cache[key]
```

> âš ï¸ **Note:** Unexpectedly slower due to cache lookup overhead in hot path.

---

#### 3. âœ… CUDA Graphs (`submission_best.py`) â€” ~30Î¼s â­ Best

Captures the entire operation sequence into a CUDA graph for replay with minimal launch overhead.

```python
# Graph capture during initialization
graph = torch.cuda.CUDAGraph()
with torch.cuda.graph(graph):
    r1 = torch._scaled_mm(a0, b1t, scale_a, scale_b1, out_dtype=torch.float32)
    r2 = torch._scaled_mm(a0, b2t, scale_a, scale_b2, out_dtype=torch.float32)
    output = (torch.nn.functional.silu(r1) * r2).half()

# Fast replay during inference
graph.replay()
```

**Why it works:**
- Eliminates per-call kernel launch latency
- Graph replay is a single GPU operation
- **3.8x speedup** over naive baseline

---

#### 4. ğŸ”„ Parallel Streams (`submission_streams.py`) â€” ~39Î¼s

Attempts to overlap GEMM1 and GEMM2 execution using separate CUDA streams.

```python
stream1, stream2 = torch.cuda.Stream(), torch.cuda.Stream()

with torch.cuda.stream(stream1):
    r1 = torch._scaled_mm(a0, b1t, ...)

with torch.cuda.stream(stream2):
    r2 = torch._scaled_mm(a0, b2t, ...)

stream1.synchronize()
stream2.synchronize()
result = F.silu(r1) * r2
```

> âš ï¸ **Limitation:** May not improve latency if individual GEMMs already saturate GPU.

---

#### 5. ğŸ“‹ Fused Dual-GEMM (Planned) â€” Target ~13Î¼s

Custom CUTLASS kernel performing both GEMMs in a single pass.

```
Our Approach:                   Fused Approach:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Load A â†’ GEMM1 â†’ Store R1       Load A once
Load A â†’ GEMM2 â†’ Store R2         â†’ GEMM1 + GEMM2 same kernel
Load R1, R2 â†’ SiLUÃ—R2 â†’ Store     â†’ SiLUÃ—R2 in registers
                                  â†’ Store C only
```

**Expected Benefits:**
| Metric | Current | Fused |
|:-------|:-------:|:-----:|
| A matrix loads | 2x | 1x |
| Kernel launches | 3 | 1 |
| Intermediate storage | 2 buffers | Registers |

---

### Performance Ranking

```
ğŸ“‹ Fused Dual-GEMM  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  ~13Î¼s (target)
âœ… CUDA Graphs      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  ~30Î¼s â­ current best
âœ… Baseline         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘  ~38Î¼s
ğŸ”„ Parallel Streams â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘  ~39Î¼s
âœ… Cached Scales    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  ~70Î¼s
```

---

## Technical Deep Dive

### The Problem

We need to compute a fused dual matrix multiplication with SiLU activation:

```
C = silu(A @ B1) * (A @ B2)

Where:
  A  = input activation  [M Ã— K]
  B1 = weight matrix 1   [N Ã— K]
  B2 = weight matrix 2   [N Ã— K]
  C  = output            [M Ã— N]
```

#### Naive Implementation (What We're Optimizing)

```
    DRAM                        GPU                         DRAM
    â”€â”€â”€â”€                        â”€â”€â”€                         â”€â”€â”€â”€

    â”Œâ”€â”€â”€â”
    â”‚ A â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  GEMM1  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º R1
    â”‚B1 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  (A@B1)                        (temp)
    â””â”€â”€â”€â”˜

    â”Œâ”€â”€â”€â”
    â”‚ A â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  GEMM2  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º R2
    â”‚B2 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  (A@B2)     â–²                  (temp)
    â””â”€â”€â”€â”˜                                â”‚
                                    A loaded TWICE!
    â”Œâ”€â”€â”€â”
    â”‚R1 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  Epilogue â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º C
    â”‚R2 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  siluÃ—mul                     (output)
    â””â”€â”€â”€â”˜
         â–²
         â””â”€â”€ R1,R2 round-trip to DRAM!
```

**Why This Is Slow:**

| Issue | Impact |
|-------|--------|
| 3 kernel launches | ~15-30Î¼s overhead |
| A loaded twice | 2Ã— memory bandwidth |
| R1, R2 round-trip | Write then read MÃ—N elements |

---

### FP4 Block-Scaled Format

#### NVFP4 (E2M1): 4-bit Floating Point

```
    FP4 E2M1 Bit Layout
    â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
    â”‚ S â”‚ E â”‚ E â”‚ M â”‚  (4 bits total)
    â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
      â”‚   â””â”€â”¬â”€â”˜   â”‚
      â”‚     â”‚     â””â”€â”€ Mantissa (1 bit)
      â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€ Exponent (2 bits)
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Sign (1 bit)

    Representable Values: {Â±0, Â±0.5, Â±1.0, Â±1.5}
```

#### Block Scaling

Every 16 FP4 elements share one FP8 scale factor:

```
    FP4 Data:     [v0][v1][v2]...[v15] [v16][v17]...[v31] ...
                   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    FP8 Scales:         [SF0]              [SF1]          ...

    Effective:    v[i] * SF[i // 16]
```

---

### Why CUDA Graphs Help

```
    WITHOUT CUDA GRAPH
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    CPU: â”€â”€[Launch K1]â”€â”€â”€â”€[Launch K2]â”€â”€â”€â”€[Launch K3]â”€â”€â”€â”€â–º
                â”‚              â”‚              â”‚
                â–¼ ~5-10Î¼s      â–¼ ~5-10Î¼s      â–¼ ~5-10Î¼s
    GPU: â”€â”€â”€â”€â”€[K1]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[K2]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[K3]â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º

    Total launch overhead: 15-30Î¼s


    WITH CUDA GRAPH
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    CPU: â”€â”€[Launch Graph]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
                â”‚
                â–¼ ~5Î¼s (single launch)
    GPU: â”€â”€â”€â”€â”€[K1]â”€[K2]â”€[K3]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º

    Total launch overhead: ~5Î¼s
    Savings: 10-25Î¼s!
```

---

### The 13Î¼s Solution

The optimal solution fuses everything into a single kernel:

```
    OPTIMAL FUSED KERNEL
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    DRAM                     GPU Registers                 DRAM
    â”€â”€â”€â”€                     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”€â”€â”€â”€

    â”Œâ”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ A â”‚â”€â”€â”€â”€â”€â–º Load once â”€â”€â–ºâ”‚ acc1 = A @ B1       â”‚
    â”‚B1 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚ acc2 = A @ B2       â”‚
    â”‚B2 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚ C = silu(acc1)*acc2 â”‚â”€â”€â”€â”€â”€â”€â–º C
    â””â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â–²
                             All in registers!
                             No intermediate DRAM!
```

**Memory Savings:**

| Operation | Naive | Fused |
|-----------|-------|-------|
| Load A | 2Ã— | 1Ã— |
| Store/Load R1,R2 | 4Ã— MÃ—N | 0 |
| **Extra traffic** | ~20MB | **0** |

---

## Q&A

<details>
<summary><strong>Why not just use PyTorch's built-in functions?</strong></summary>

We do! `torch._scaled_mm` uses cuBLAS FP4 tensor cores internally. The optimization is about reducing the overhead *around* these calls â€” launch latency, memory traffic, and redundant operations.

</details>

<details>
<summary><strong>Why is CUDA graph faster than raw PyTorch?</strong></summary>

CUDA graphs eliminate kernel launch overhead (~5-10Î¼s per kernel). With 3 kernels in the naive approach, that's 15-30Î¼s of pure overhead eliminated by capturing and replaying the graph.

</details>

<details>
<summary><strong>Can the two GEMMs run in parallel?</strong></summary>

Theoretically yes with CUDA streams, but FP4 GEMMs likely saturate the tensor cores entirely. When compute is fully utilized, parallelism at the stream level doesn't help â€” the GPU can only do one thing at a time anyway.

</details>

<details>
<summary><strong>Why FP32 intermediate instead of FP16?</strong></summary>

Precision. The SiLU activation (`x * sigmoid(x)`) involves an exponential operation that benefits from FP32 accumulation. FP16 intermediates cause numerical errors that fail validation.

</details>

<details>
<summary><strong>What would it take to reach 13Î¼s?</strong></summary>

A fully fused kernel that:
1. Loads A tiles once and reuses for both B1 and B2
2. Keeps both accumulator results in registers
3. Computes `silu(acc1) * acc2` without touching DRAM
4. Writes only the final output C

This requires a custom CUTLASS kernel with modified mainloop and epilogue.

</details>

<details>
<summary><strong>Why is scale factor conversion so complex?</strong></summary>

cuBLAS uses a specific "atom" layout for FP4 block scales that's optimized for tensor core access patterns. The task provides scale factors in a different permuted format. Converting between them requires careful index manipulation: `[32, 4, M//128, 4, K//64, L]` â†” flattened blocked format.

</details>

---

## Lessons Learned

| | Insight |
|:---:|---------|
| ğŸ’¡ | **Kernel launch overhead dominates** small matrix operations â€” profile before assuming compute is the bottleneck |
| ğŸ’¡ | **CUDA graphs are powerful** but require static shapes and careful memory management |
| ğŸ’¡ | **Memory bandwidth is the bottleneck**, not compute â€” fusing operations to reduce DRAM traffic is key |
| ğŸ’¡ | **Scale factor layouts differ** between libraries â€” PyTorch/cuBLAS vs CUTLASS expect different formats |
| ğŸ’¡ | **Sometimes simple beats complex** â€” our PyTorch + CUDA graph solution outperformed initial CUTLASS attempts |
| ğŸ’¡ | **Profile first, optimize second** â€” assumptions about bottlenecks are often wrong |

---

## Future Work

- [ ] Implement fused dual-GEMM CUTLASS kernel for 13Î¼s target
- [ ] Explore Triton for custom kernel fusion
- [ ] Benchmark across different problem sizes (M, N, K)
- [ ] Add FP8 variant for comparison
- [ ] Profile memory bandwidth utilization

---

## Project Structure

```
nvfp4_dual_gemm/
â”œâ”€â”€ python/
â”‚   â”œâ”€â”€ submission.py          # Baseline implementation
â”‚   â”œâ”€â”€ submission_v2.py       # Cached scale factors
â”‚   â”œâ”€â”€ submission_best.py     # CUDA graph optimized â­
â”‚   â”œâ”€â”€ submission_streams.py  # Parallel streams experiment
â”‚   â””â”€â”€ task.py                # Task definition & validation
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ dual_gemm_nvfp4.cu     # CUTLASS kernel (WIP)
â”‚   â”œâ”€â”€ nvfp4_gemm.cuh         # GEMM wrapper
â”‚   â””â”€â”€ silu_mul_kernel.cuh    # Fused epilogue
â”œâ”€â”€ cutlass/                   # CUTLASS library (submodule)
â””â”€â”€ README.md                  # This file
```

---

## Usage

```python
from python.submission_best import custom_kernel

# Input tuple: (a, b1, b2, sfa, sfb1, sfb2, sfa_perm, sfb1_perm, sfb2_perm, c_out)
result = custom_kernel(data)
```

---

## Acknowledgments

Built for the [GPU MODE](https://gpumode.com/) NVFP4 Dual-GEMM challenge.

Target hardware: **NVIDIA B200 (Blackwell)** with SM100 FP4 Tensor Cores.

---

<p align="center">
  <sub>Made with âš¡ and lots of profiling</sub>
</p>
