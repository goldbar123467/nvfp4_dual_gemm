<div align="center">

```
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•—
â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘ â–ˆâ–ˆâ•”â•    â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â–ˆâ–ˆâ•”â•
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•        â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•
â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•—        â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•—
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•—       â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•—
â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â•       â•šâ•â•   â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â•â•â•â•šâ•â•  â•šâ•â•

         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
        â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•
        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
        â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•
        â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘    â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
        â•šâ•â•  â•šâ•â•â•šâ•â•     â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•
```

# ğŸ¦ˆ AI Agents Compete to Optimize CUDA Kernels

### A GPUMode Leaderboard Challenge Solved Through AI Agent Competition

</div>

<p align="center">
  <a href="#"><img src="https://img.shields.io/badge/CHALLENGE-GPUMode-ff6600?style=for-the-badge" alt="GPUMode"></a>
  <a href="#"><img src="https://img.shields.io/badge/METHOD-AI_Shark_Tank-blue?style=for-the-badge" alt="AI Shark Tank"></a>
  <a href="#"><img src="https://img.shields.io/badge/GPU-B200_BLACKWELL-76B900?style=for-the-badge&logo=nvidia" alt="B200"></a>
  <a href="#"><img src="https://img.shields.io/badge/POWERED_BY-Claude_Code-purple?style=for-the-badge" alt="Claude Code"></a>
</p>

---

## ğŸ¯ The Challenge

**GPUMode Leaderboard**: Implement a high-performance NVFP4 Group GEMM kernel for NVIDIA B200 (Blackwell) GPUs.

```
C = A @ B  (with FP4 quantization and block scaling)
```

**The Twist**: We solved it using **AI Agent Competition** â€” multiple Claude agents pitch optimizations, and a panel of "Shark" agents vote on which approach to implement.

---

## ğŸ† The Shark Tank Format

Instead of one AI blindly trying optimizations, we created a **competitive framework**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      SHARK TANK ROUND                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   CONTESTANTS (4 AI Agents)          SHARKS (3 AI Agents)       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚ Pitch 1: Approach A â”‚            â”‚ Shark 1: Skeptic    â”‚    â”‚
â”‚   â”‚ Pitch 2: Approach B â”‚  â”€â”€â”€â”€â”€â”€â–º   â”‚ Shark 2: Pragmatist â”‚    â”‚
â”‚   â”‚ Pitch 3: Approach C â”‚            â”‚ Shark 3: Theorist   â”‚    â”‚
â”‚   â”‚ Pitch 4: Approach D â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚                 â”‚
â”‚                                               â–¼                 â”‚
â”‚                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚                                        â”‚   VOTE    â”‚            â”‚
â”‚                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                               â”‚                 â”‚
â”‚                                               â–¼                 â”‚
â”‚                                     IMPLEMENT WINNER            â”‚
â”‚                                               â”‚                 â”‚
â”‚                                               â–¼                 â”‚
â”‚                                         BENCHMARK               â”‚
â”‚                                               â”‚                 â”‚
â”‚                                               â–¼                 â”‚
â”‚                                        NEXT ROUND               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why This Works

1. **Diverse Perspectives**: Each contestant agent proposes a different optimization strategy
2. **Critical Evaluation**: Shark agents scrutinize claims with different lenses (skepticism, practicality, theory)
3. **Fail Fast**: Bad ideas get voted down before wasting implementation time
4. **Learning Accumulates**: Each round's learnings inform the next

---

## ğŸ“Š Season Results

| Round | Contestants | Winner | Expected | Actual | Status |
|-------|-------------|--------|----------|--------|--------|
| **1** | Pipeline Stages, Tile Tuning, TMA Epilogue, Warp Spec | Pipeline Stages | 1.5x faster | **30% SLOWER** | âŒ FAILED |
| **2** | Tile 64x128, Tile 128x64, Stage=2, Stage=4 | Tile Size Tuning | 2-3x faster | **COMPILE ERROR** | âŒ FAILED |
| **3** | Triton Rewrite, cuBLAS, Wild Card Debug | Wild Card Debug | ??? | **Found the bug!** | âœ… SUCCESS |
| **4** | Dual GEMM Fusion, Two-Pass Fix, Interleaved | Minimal Two-Pass | Correctness | **Fixed kernel** | âœ… SUCCESS |
| **5** | Triton, torch.compile, Stream Parallelism | Stream Parallelism | 4-7x faster | **NOT ALLOWED** | âš ï¸ BLOCKED |
| **6** | Persistent Kernel, Warp Spec, Split-K, Reduce Overhead | **Reduce Overhead** | 6-19x faster | **TBD** | ğŸ”„ IN PROGRESS |

---

## ğŸ” Key Discoveries

### Round 3: The Bug Hunt ğŸ›

The kernel was computing the **wrong thing**:
```python
# What the kernel computed:
C = A @ B

# What the task required:
C = silu(A @ B1) * (A @ B2)  # Dual GEMM with SiLU fusion!
```

A "Wild Card" contestant discovered this by actually reading the task specification.

### Round 5: Competition Rules Matter âš ï¸

Stream parallelism would have given 4-7x speedup, but **GPUMode forbids multiple CUDA streams**:
```
âŒ "Your code contains work on another stream"
```

### Round 6: Python is the Bottleneck ğŸ

The unanimous winner discovered that **Python overhead**, not CUDA, was the problem:

```python
# BEFORE: 50Âµs overhead per call
tensor_of_abc_ptrs = torch.tensor(abc_ptrs, device="cuda")  # ~15Âµs
tensor_of_sfasfb_ptrs = torch.tensor(sfasfb_ptrs, device="cuda")  # ~15Âµs
tensor_of_problem_sizes = torch.tensor(problem_sizes, device="cuda")  # ~15Âµs

# AFTER: <5Âµs overhead per call
cache = get_cached_metadata_tensors(num_groups, total_clusters)
cache['abc_ptrs'].copy_(cache['abc_ptrs_cpu'], non_blocking=True)  # ~1Âµs
```

---

## ğŸ“ Repository Structure

```
nvfp4_dual_gemm_repo/
â”œâ”€â”€ README.md                    # You are here
â”œâ”€â”€ nvfp4_group_gemm/
â”‚   â”œâ”€â”€ submission.py            # Current best submission
â”‚   â”œâ”€â”€ submission_v8_prealloc.py # Round 6 winner (pre-allocation)
â”‚   â””â”€â”€ submission_v7_final.py   # Previous version
â”œâ”€â”€ shark_tank/
â”‚   â”œâ”€â”€ rounds/
â”‚   â”‚   â”œâ”€â”€ round1_results.md    # Pipeline stages (FAILED)
â”‚   â”‚   â”œâ”€â”€ round2_results.md    # Tile tuning (FAILED)
â”‚   â”‚   â”œâ”€â”€ round3_results.md    # Bug discovery (SUCCESS)
â”‚   â”‚   â”œâ”€â”€ round4_results.md    # Two-pass fix (SUCCESS)
â”‚   â”‚   â”œâ”€â”€ round5_results.md    # Streams (BLOCKED)
â”‚   â”‚   â””â”€â”€ round6_results.md    # Pre-allocation (IN PROGRESS)
â”‚   â””â”€â”€ pitches/                 # Individual pitch documents
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ PITCH_REDUCE_LAUNCH_OVERHEAD.md
â”œâ”€â”€ SHARK_TANK_LEARNINGS.md      # Accumulated wisdom
â””â”€â”€ task.md                      # Original challenge spec
```

---

## ğŸ¦ˆ The Sharks

Each round features three AI "Shark" evaluators with distinct personalities:

| Shark | Personality | Focus |
|-------|-------------|-------|
| **The Skeptic** | "Prove it works" | Demands evidence, distrusts claims |
| **The Pragmatist** | "Can we ship it?" | Implementation feasibility, quick wins |
| **The Theorist** | "Why does this happen?" | Root cause analysis, mathematical proof |

### Example Voting (Round 6)

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ROUND 6 WINNER: REDUCE LAUNCH OVERHEAD                        â•‘
â•‘  UNANIMOUS VOTE (3-0)                                          â•‘
â•‘                                                                â•‘
â•‘  Skeptic:    "Finally attacking the actual bottleneck."        â•‘
â•‘  Pragmatist: "4-6 hours, low risk, 6-19x upside."              â•‘
â•‘  Theorist:   "The math checks out: 60Âµs/group with 2-5Âµs       â•‘
â•‘              compute = 50Âµs Python overhead."                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸš€ Performance Journey

```
Starting Point:     ~530 Âµs
After Round 4:      ~479 Âµs  (fixed correctness)
After Round 6:      ~50-80 Âµs (expected, pending benchmark)
Target:             ~18.8 Âµs

Progress: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ 80% (if Round 6 works)
```

---

## ğŸ’¡ Learnings

### What Doesn't Work on B200 NVFP4

| Optimization | Why It Failed |
|--------------|---------------|
| Pipeline stages (num_ab_stage=3) | Compute-bound, not memory-bound |
| Smaller tiles (64x128) | Hardware requires 128x128 minimum |
| Multiple CUDA streams | Competition rules forbid it |
| Triton rewrite | Can't access NVFP4 MMA instructions |

### What Does Work

| Optimization | Why It Works |
|--------------|--------------|
| Pre-allocated tensor cache | Eliminates Python overhead |
| Pinned memory + async copy | Fast CPUâ†’GPU transfer |
| Kernel compilation caching | Avoids JIT overhead |

---

## ğŸ› ï¸ How to Run Your Own Shark Tank

1. **Define the Challenge**: Clear metrics, constraints, and targets
2. **Spawn Contestants**: 3-4 AI agents with different optimization approaches
3. **Spawn Sharks**: 3 AI agents with different evaluation criteria
4. **Let Them Debate**: Contestants pitch, sharks critique and vote
5. **Implement Winner**: Build only the winning approach
6. **Benchmark**: Measure actual performance
7. **Repeat**: Use learnings to inform next round

```python
# Pseudo-code for running a Shark Tank round
contestants = [
    Agent("Pitch A: Persistent Kernel"),
    Agent("Pitch B: Warp Specialization"),
    Agent("Pitch C: Split-K"),
    Agent("Pitch D: Reduce Overhead"),
]

sharks = [
    Agent("Skeptic", personality="demands proof"),
    Agent("Pragmatist", personality="wants quick wins"),
    Agent("Theorist", personality="needs math"),
]

pitches = [c.generate_pitch(context) for c in contestants]
votes = [s.evaluate_and_vote(pitches) for s in sharks]
winner = majority_vote(votes)

implement(winner)
benchmark()
```

---

## ğŸ“œ License

MIT

---

## ğŸ™ Acknowledgments

- **GPUMode** for the challenge and leaderboard
- **NVIDIA** for B200 hardware and CUTLASS/CuTe DSL
- **Anthropic** for Claude Code that powers the AI agents

---

<div align="center">

*"The best optimization isn't the one that sounds cleverâ€”it's the one that survives scrutiny from three skeptical AIs."*

**â€” Shark Tank AI Methodology**

</div>
